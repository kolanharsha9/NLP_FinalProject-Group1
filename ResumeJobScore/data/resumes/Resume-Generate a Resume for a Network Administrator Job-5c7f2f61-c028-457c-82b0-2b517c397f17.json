{
    "clean_data": "Data Scientist Data Scientist Data Scientist Zoetis Inc Data Scientist with 6 Plus years of experience in Statistical Modeling Machine Learning Data Mining with Structured and Unstructured Data Performed Data Acquisition Data Validation Predictive Modeling and Data Visualization Expertise in Python 2x3x programming with multiple packages including NumPy Pandas Matplotlib SciPy Seaborn and Scikitlearn Hands on experience in implementing LDA Nave Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering Neural Networks Principle Component Analysis and good knowledge on Recommender Systems Implementation experiences in Machine Learning and deep learning including Regression Classification Natural Language Processing NLP using packages like NLTK Spacy Experience in tuning algorithms using methods such as Grid Search Randomized Search KFold Cross Validation and Error Analysis Also worked with several boosting methodologies like ADA Boost Gradient Boosting and XGBoost Validated the machine learning classifiers using Accuracy AUC ROC Curves and Lift Charts Worked with various text analytics or Word Embedding libraries like Word2Vec Count Vectorizer GloVe LDA etc Solid knowledge and experience in Deep Learning techniques including Feedforward Neural Network Convolutional Neural Network CNN Recursive Neural Network RNN Worked with numerous data visualization tools in python like Matplotlib Seaborn ggplot pygal Worked and extracted data from various database sources like Oracle SQL Serverand MongoDB Highly skilled in using Hadoop HBase Spark and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Experience on working with different operating systems like UNIX LINUX and Windows Experience working with MS Word MS Excel MS PowerPoint MS SharePoint and MS Project Work Experience Data Scientist Zoetis Inc September 2018 to Present Zoetis Inc is the worlds largest producer of medicine and vaccinations for pets and livestock Zoetis delivers quality medicines vaccines and diagnostic products which are complemented by genetic tests bio devices and a range of services The project is to collect data from different sources and create a master data set and doing predictions on sales and profits Measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products Responsibilities Developing analytical databases from different sources and create a master data set Responsible for data identification collection exploration cleaning for modeling Performed Time Series Analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values Determined customer satisfaction and helped enhance customer experience using NLP Manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products Analysis of biological and spatial data to develop insights into precision animal management and precision medicine Implementing analytics algorithms in Python Performed training Natural Language models and reinforcement learning engines to optimize intelligent agents that automate task execution Performed kMeans clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering customized and priority service to improve existing profitable relationships and to avoid customer churn etc using Python Performed Text analytics on unstructured email data using Natural language processing tool kit NLTK Worked with text to vector representation methods including Counter Vectorizer Tfidf for topic modelling Applying Clustering algorithms to group the data on their similar behavior patterns Work with data analytics team to develop time series and optimization Experienced in data scraping Used PySpark Machine learning library to build and evaluate different models Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Experienced in Agile Methodology Used Tableau to generate reports with internal records secondary sources of data JSON CSV and more Which helped the support team for better marketing Environment Python 370 PySpark NLTK SQL Server Microsoft Excel SQL AWS Tablau Sqoop ETL agile Data Scientist Mars Solutions Group WI March 2017 to August 2018 The Client is the largest Healthcare Provider and offers health care products insurance services Data Analytics Payment Integrity and The project was to build predictive models for customer value analysis by applying machine learning methods principal component analysis and regression on large data set Responsibilities Worked on Machine Learning Data mining with large data sets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Performed Multinomial Logistic Regression Random forest machine learning algorithms Used AWS to manage the data in cloud Good knowledge on Hadoop components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Maintained updated Log files using Python Alteryx was used for Data Preparation Used machine learning algorithms like Logistic Regression Knn Decision Trees Random Forest to make the data to fit for the desired output Interact and brainstorm with multifunctional teams to explore the opportunities of using data to improve business and healthcare outcomes Developed machinelearning models and translate complex ideas and results into actionable management insights and solutions to achieve the expected businesshealthcare goals Design coding unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database Worked in Agile Methodology Generated reports with internal records secondary sources of data JSON CSV and more Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Provided schedules status reports and issue resolutions to the Project team Business Users and Project Managers Environment Python 3x Linux Spark SQL Server 2012 Microsoft Excel Spark SQL AWS QlikView Sqoop ETL agile Data Analyst Data Scientist CMS Energy Jackson MI January 2016 to February 2017 CMS Energy is an energy company that is focused principally on utility operations I was responsible for building a new data science department with the help of other departments and I was able to learn how the business is operated and helped the company to grow and stay ahead of the competition By using machine learning we improvised the predictive algorithm for pricing strategy And we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me Responsibilities Worked on Data Manipulation Visualization Machine Learning Python and SQL Transformed the business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Worked on customer segmentation using an unsupervised learning technique clustering Implemented Classification using Supervised learning like Logistic Regression Decision trees KNN Naive Bayes Built models using Statistical techniques and Machine Learning classification models like XG Boost SVM and Random Forest Improved models accuracy by using Gradient Boosting technique like Light GBM and gained around 82 accuracy with Random Forest and 77 with Logistic Regression Used Jupyter notebook for spark to make data manipulations Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs Environment Tableau 1005 AWS GIT Python 352 AnacondaNavigator Hadoop NoSQL random forest MongoDB HDFS NLTK XML MapReduce Informatica Data Analyst Karvy Financial Services Limited November 2014 to December 2015 Karvy Financial Services Limited is a company which has been playing a very proactive role in the economic growth of India by providing loans to Micro Small Business segments and individuals like credit for the requirements of different sectors of economy Industries exports trading agriculture infrastructure and the individual segments We worked on various projects which handles customer analytics Credit Risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans identify and prevent fraud detection for transactions Responsibilities Compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results Applied concepts of probability distribution and statistical inference on the given dataset to unearth interesting findings using comparison Ttest Ftest Rsquared Pvalue etc Applied linear regression multiple regression ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied Principal Component Analysis PCA based unsupervised technique to determine unusual VPN logon time Performed Clustering with historical demographic and behavioral data as features to implement the personalized marketing to the customers Also created classification model using Logistic Regression Random Forests to classify dependent variable into two classes which are risky and okay Used FScore Precision recall evaluating model performance Built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as Decision trees Random Forests and SVM Real time analysis of customer financial profile and providing recommendation for financial products best suited Performed Sentimental analysis NLP on the email feedback of the customers to determine the emotional tone behind the series of words and gain the express of the attitudes and emotions by LongShort Term Memory LSTM cells in Recurrent Neural NetworksRNN Forecasted demand for loans and interest rates using Time Series analysis like ARIMAX VARMAX and HoltWinters Obtained better predictive performance of 81 accuracy using ensemble methods like Bootstrap aggregation Bagging and Boosting Light GBM Gradient Tested complex ETL mappings and sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Performed financial data ingestion to the Spark distribution environment using Kafka Developed visualizations and dashboards using ggplot Tableau Prepared and presented data quality report to stakeholders to give understanding of data Environment Tableau 1005 GIT Python 352 AnacondaNavigator Hadoop Spark Kafka NoSQL Random forest MongoDB HDFS NLP Python Developer Data Analyst Symbiosys Technologies Visakhapatnam Andhra Pradesh January 2014 to October 2014 Genius Brands International is our client and we performed exploratory data analysis on corporate purchase orders contracts and projects data using sampling and statistical methods Identified strata improved precision and accuracy Works with other team members including DBAs Other ETL developers Technical Architects QA and Business Analysts Project Managers Responsibilities Participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Used Python to place data into JSON files for testing Django Websites Updated and manipulated content and files by using python scripts Worked on Python Open stack APIs Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Implemented endtoend systems for Data Analytics Data Automation and customized visualization tools using Python R Hadoop and MongoDB Used pandas NumPy seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Worked on csv json excel different types of files for the data cleaning and data analysis Used R for statistical operations on the data and ggplot2 for the visualizing the data Application of various ML algorithms and statistical modeling like decision trees regression models random forest SVM clustering to identify Volume using scikitlearn package in python Performed Classification using Supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Performed data profiling to merge the data from multiple data sources Extracted data from HDFS Hadoop Distributed File System and prepared data for exploratory analysis using data munging Performed time series analysis using Tableau Knowledge of other relational database platforms such as Oracle DB2 NoSQL Managed offshore projects and coordinated work for 24 hour productivity cycle Environment Python 27 Django 14 R Oracle GitHub SQL Server HDFS Hive ETL Developer Sutherland Global Services Hyderabad Telangana February 2013 to December 2013 Sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics Sutherland has customers across industries like financial services to Healthcare My role is to assist Analytics department for the data extraction and cleaning as a data preprocessing steps to build models Responsibilities Involved with Business Analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Managed full SDLC processes involving requirements management workflow analysis source data analysis data mapping metadata management data quality testing strategy and maintenance of the model Developed ETLs to pull data from various sources and transform it for reporting applications using PLSQL Designed SSIS packages to extract transform and load existing data into SQL Server used lots of components of SSIS such as Pivot Transformation Fuzzy Lookup Merge Merge Join Data Conversion Row Count Sort Derived Columns Conditional Split Execute SQL Task Data Flow Task and Execute Package Task Created SSIS Packages that involved dealing with different source formats flat files Excel XML and different destination formats Debugged and troubleshot the ETL packages by using a breakpoint analyzing the process catching error information by SQL command in SSIS Developed SQL queries in SQL Server management studio Toad and generated complex reports forth end users Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD BTEQ and Fast Load Experience with Perl Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Environment ETL Tools SDLC GitHub SQL Server PLSQL Excel XML SQL Education Master of Science in Information Technology Management in Information Technology Management Campbellsville University Bachelors in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University",
    "entities": [
        "Natural",
        "Performed Sentimental",
        "Informatica",
        "Python Applied Principal Component Analysis",
        "Data Analytics Payment Integrity",
        "Logistic Regression Knn Decision Trees Random Forest",
        "Accuracy AUC ROC",
        "KNN Naive Bayes Built",
        "UNIX",
        "Sutherland",
        "Performed Time Series Analysis",
        "Tablau Sqoop ETL",
        "Data Analyst Data",
        "PySpark Machine",
        "Regression Classification Natural Language Processing",
        "KNN Naive",
        "metadata",
        "XG Boost SVM",
        "Hadoop",
        "Tableau Knowledge",
        "Nehru Technological University",
        "Telangana",
        "Perl Performed",
        "Maintained",
        "Micro Small Business",
        "Automated",
        "Random Forests Decision Trees Linear and Logistic Regression SVM Clustering Neural Networks Principle Component Analysis",
        "SSIS",
        "SQL Server",
        "Developed",
        "Performed Classification",
        "Implemented Classification",
        "Node Data",
        "Feedforward Neural Network",
        "Oracle and Teradata Environment",
        "Healthcare",
        "Recommender Systems Implementation",
        "Ttest Ftest Rsquared Pvalue",
        "Oracle SQL Serverand",
        "Statistical Modeling Machine Learning Data Mining with",
        "Project Managers Environment",
        "PowerPoint MS SharePoint",
        "MS Project Work Experience Data Scientist Zoetis Inc",
        "CMS Energy",
        "Technical Architects QA",
        "Works",
        "Unstructured",
        "ADA",
        "Unstructured Data Performed Data Acquisition Data Validation Predictive Modeling",
        "Built",
        "GitHub SQL Server",
        "Python R Hadoop",
        "Data Preparation",
        "VARMAX",
        "Genius Brands International",
        "Hadoop HBase Spark",
        "Credit Risk",
        "Spark",
        "Statistical",
        "Business Analysts Project Managers Responsibilities Participated",
        "Responsibilities Worked on Data Manipulation Visualization Machine Learning Python",
        "Developed ETL",
        "INFORMATICA",
        "AWS",
        "Financial Services Limited",
        "Oracle",
        "Developer Sutherland Global Services Hyderabad",
        "Science in Information Technology Management in Information Technology Management Campbellsville University Bachelors",
        "MS Word MS Excel",
        "NoSQL Random",
        "Random Forest",
        "Responsibilities Involved with Business Analysts",
        "HDFS Job Tracker Task Tracker",
        "Data Acquisition Data Validation Predictive",
        "SQL",
        "GBM",
        "Bootstrap",
        "NLP Manage",
        "NLP",
        "HDFS Hadoop Distributed File System",
        "Recurrent Neural NetworksRNN Forecasted",
        "Hive",
        "Karvy Financial Services Limited",
        "Pandas",
        "Present Zoetis Inc",
        "ETL",
        "India",
        "Performed",
        "PySpark NLTK",
        "SSIS Developed",
        "Logistic Regression Decision",
        "Convolutional Neural Network",
        "Microsoft",
        "Data Analytics Data Automation",
        "Execute Package Task Created",
        "Deep Learning",
        "ML",
        "Data Scientist Data Scientist Data",
        "Data Scientist Mars Solutions Group",
        "Random Forest Improved",
        "Structured",
        "Toad",
        "Responsibilities Worked on Machine Learning Data",
        "MapReduce",
        "Healthcare Provider",
        "Machine Learning",
        "Communication Engineering in",
        "Pivot Transformation",
        "Data Visualization Expertise",
        "SVM"
    ],
    "experience": "Experience in tuning algorithms using methods such as Grid Search Randomized Search KFold Cross Validation and Error Analysis Also worked with several boosting methodologies like ADA Boost Gradient Boosting and XGBoost Validated the machine learning classifiers using Accuracy AUC ROC Curves and Lift Charts Worked with various text analytics or Word Embedding libraries like Word2Vec Count Vectorizer GloVe LDA etc Solid knowledge and experience in Deep Learning techniques including Feedforward Neural Network Convolutional Neural Network CNN Recursive Neural Network RNN Worked with numerous data visualization tools in python like Matplotlib Seaborn ggplot pygal Worked and extracted data from various database sources like Oracle SQL Serverand MongoDB Highly skilled in using Hadoop HBase Spark and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Experience on working with different operating systems like UNIX LINUX and Windows Experience working with MS Word MS Excel MS PowerPoint MS SharePoint and MS Project Work Experience Data Scientist Zoetis Inc September 2018 to Present Zoetis Inc is the worlds largest producer of medicine and vaccinations for pets and livestock Zoetis delivers quality medicines vaccines and diagnostic products which are complemented by genetic tests bio devices and a range of services The project is to collect data from different sources and create a master data set and doing predictions on sales and profits Measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products Responsibilities Developing analytical databases from different sources and create a master data set Responsible for data identification collection exploration cleaning for modeling Performed Time Series Analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values Determined customer satisfaction and helped enhance customer experience using NLP Manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products Analysis of biological and spatial data to develop insights into precision animal management and precision medicine Implementing analytics algorithms in Python Performed training Natural Language models and reinforcement learning engines to optimize intelligent agents that automate task execution Performed kMeans clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering customized and priority service to improve existing profitable relationships and to avoid customer churn etc using Python Performed Text analytics on unstructured email data using Natural language processing tool kit NLTK Worked with text to vector representation methods including Counter Vectorizer Tfidf for topic modelling Applying Clustering algorithms to group the data on their similar behavior patterns Work with data analytics team to develop time series and optimization Experienced in data scraping Used PySpark Machine learning library to build and evaluate different models Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Experienced in Agile Methodology Used Tableau to generate reports with internal records secondary sources of data JSON CSV and more Which helped the support team for better marketing Environment Python 370 PySpark NLTK SQL Server Microsoft Excel SQL AWS Tablau Sqoop ETL agile Data Scientist Mars Solutions Group WI March 2017 to August 2018 The Client is the largest Healthcare Provider and offers health care products insurance services Data Analytics Payment Integrity and The project was to build predictive models for customer value analysis by applying machine learning methods principal component analysis and regression on large data set Responsibilities Worked on Machine Learning Data mining with large data sets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Performed Multinomial Logistic Regression Random forest machine learning algorithms Used AWS to manage the data in cloud Good knowledge on Hadoop components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Maintained updated Log files using Python Alteryx was used for Data Preparation Used machine learning algorithms like Logistic Regression Knn Decision Trees Random Forest to make the data to fit for the desired output Interact and brainstorm with multifunctional teams to explore the opportunities of using data to improve business and healthcare outcomes Developed machinelearning models and translate complex ideas and results into actionable management insights and solutions to achieve the expected businesshealthcare goals Design coding unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database Worked in Agile Methodology Generated reports with internal records secondary sources of data JSON CSV and more Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Provided schedules status reports and issue resolutions to the Project team Business Users and Project Managers Environment Python 3x Linux Spark SQL Server 2012 Microsoft Excel Spark SQL AWS QlikView Sqoop ETL agile Data Analyst Data Scientist CMS Energy Jackson MI January 2016 to February 2017 CMS Energy is an energy company that is focused principally on utility operations I was responsible for building a new data science department with the help of other departments and I was able to learn how the business is operated and helped the company to grow and stay ahead of the competition By using machine learning we improvised the predictive algorithm for pricing strategy And we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me Responsibilities Worked on Data Manipulation Visualization Machine Learning Python and SQL Transformed the business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Worked on customer segmentation using an unsupervised learning technique clustering Implemented Classification using Supervised learning like Logistic Regression Decision trees KNN Naive Bayes Built models using Statistical techniques and Machine Learning classification models like XG Boost SVM and Random Forest Improved models accuracy by using Gradient Boosting technique like Light GBM and gained around 82 accuracy with Random Forest and 77 with Logistic Regression Used Jupyter notebook for spark to make data manipulations Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs Environment Tableau 1005 AWS GIT Python 352 AnacondaNavigator Hadoop NoSQL random forest MongoDB HDFS NLTK XML MapReduce Informatica Data Analyst Karvy Financial Services Limited November 2014 to December 2015 Karvy Financial Services Limited is a company which has been playing a very proactive role in the economic growth of India by providing loans to Micro Small Business segments and individuals like credit for the requirements of different sectors of economy Industries exports trading agriculture infrastructure and the individual segments We worked on various projects which handles customer analytics Credit Risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans identify and prevent fraud detection for transactions Responsibilities Compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results Applied concepts of probability distribution and statistical inference on the given dataset to unearth interesting findings using comparison Ttest Ftest Rsquared Pvalue etc Applied linear regression multiple regression ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied Principal Component Analysis PCA based unsupervised technique to determine unusual VPN logon time Performed Clustering with historical demographic and behavioral data as features to implement the personalized marketing to the customers Also created classification model using Logistic Regression Random Forests to classify dependent variable into two classes which are risky and okay Used FScore Precision recall evaluating model performance Built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as Decision trees Random Forests and SVM Real time analysis of customer financial profile and providing recommendation for financial products best suited Performed Sentimental analysis NLP on the email feedback of the customers to determine the emotional tone behind the series of words and gain the express of the attitudes and emotions by LongShort Term Memory LSTM cells in Recurrent Neural NetworksRNN Forecasted demand for loans and interest rates using Time Series analysis like ARIMAX VARMAX and HoltWinters Obtained better predictive performance of 81 accuracy using ensemble methods like Bootstrap aggregation Bagging and Boosting Light GBM Gradient Tested complex ETL mappings and sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Performed financial data ingestion to the Spark distribution environment using Kafka Developed visualizations and dashboards using ggplot Tableau Prepared and presented data quality report to stakeholders to give understanding of data Environment Tableau 1005 GIT Python 352 AnacondaNavigator Hadoop Spark Kafka NoSQL Random forest MongoDB HDFS NLP Python Developer Data Analyst Symbiosys Technologies Visakhapatnam Andhra Pradesh January 2014 to October 2014 Genius Brands International is our client and we performed exploratory data analysis on corporate purchase orders contracts and projects data using sampling and statistical methods Identified strata improved precision and accuracy Works with other team members including DBAs Other ETL developers Technical Architects QA and Business Analysts Project Managers Responsibilities Participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Used Python to place data into JSON files for testing Django Websites Updated and manipulated content and files by using python scripts Worked on Python Open stack APIs Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Implemented endtoend systems for Data Analytics Data Automation and customized visualization tools using Python R Hadoop and MongoDB Used pandas NumPy seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Worked on csv json excel different types of files for the data cleaning and data analysis Used R for statistical operations on the data and ggplot2 for the visualizing the data Application of various ML algorithms and statistical modeling like decision trees regression models random forest SVM clustering to identify Volume using scikitlearn package in python Performed Classification using Supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Performed data profiling to merge the data from multiple data sources Extracted data from HDFS Hadoop Distributed File System and prepared data for exploratory analysis using data munging Performed time series analysis using Tableau Knowledge of other relational database platforms such as Oracle DB2 NoSQL Managed offshore projects and coordinated work for 24 hour productivity cycle Environment Python 27 Django 14 R Oracle GitHub SQL Server HDFS Hive ETL Developer Sutherland Global Services Hyderabad Telangana February 2013 to December 2013 Sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics Sutherland has customers across industries like financial services to Healthcare My role is to assist Analytics department for the data extraction and cleaning as a data preprocessing steps to build models Responsibilities Involved with Business Analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Managed full SDLC processes involving requirements management workflow analysis source data analysis data mapping metadata management data quality testing strategy and maintenance of the model Developed ETLs to pull data from various sources and transform it for reporting applications using PLSQL Designed SSIS packages to extract transform and load existing data into SQL Server used lots of components of SSIS such as Pivot Transformation Fuzzy Lookup Merge Merge Join Data Conversion Row Count Sort Derived Columns Conditional Split Execute SQL Task Data Flow Task and Execute Package Task Created SSIS Packages that involved dealing with different source formats flat files Excel XML and different destination formats Debugged and troubleshot the ETL packages by using a breakpoint analyzing the process catching error information by SQL command in SSIS Developed SQL queries in SQL Server management studio Toad and generated complex reports forth end users Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD BTEQ and Fast Load Experience with Perl Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Environment ETL Tools SDLC GitHub SQL Server PLSQL Excel XML SQL Education Master of Science in Information Technology Management in Information Technology Management Campbellsville University Bachelors in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Zoetis",
        "Inc",
        "Data",
        "Scientist",
        "years",
        "experience",
        "Statistical",
        "Modeling",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "Structured",
        "Unstructured",
        "Data",
        "Performed",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "Modeling",
        "Data",
        "Visualization",
        "Expertise",
        "Python",
        "2x3x",
        "programming",
        "packages",
        "NumPy",
        "Pandas",
        "Matplotlib",
        "SciPy",
        "Seaborn",
        "Scikitlearn",
        "Hands",
        "experience",
        "LDA",
        "Nave",
        "Bayes",
        "Random",
        "Forests",
        "Decision",
        "Trees",
        "Linear",
        "Logistic",
        "Regression",
        "SVM",
        "Clustering",
        "Neural",
        "Networks",
        "Principle",
        "Component",
        "Analysis",
        "knowledge",
        "Recommender",
        "Systems",
        "Implementation",
        "experiences",
        "Machine",
        "Learning",
        "learning",
        "Regression",
        "Classification",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "packages",
        "NLTK",
        "Spacy",
        "Experience",
        "algorithms",
        "methods",
        "Grid",
        "Search",
        "Randomized",
        "Search",
        "KFold",
        "Cross",
        "Validation",
        "Error",
        "Analysis",
        "methodologies",
        "ADA",
        "Boost",
        "Gradient",
        "Boosting",
        "XGBoost",
        "machine",
        "classifiers",
        "Accuracy",
        "AUC",
        "ROC",
        "Curves",
        "Lift",
        "Charts",
        "text",
        "analytics",
        "Word",
        "libraries",
        "Word2Vec",
        "Count",
        "Vectorizer",
        "GloVe",
        "LDA",
        "knowledge",
        "experience",
        "Deep",
        "Learning",
        "techniques",
        "Feedforward",
        "Neural",
        "Network",
        "Convolutional",
        "Neural",
        "Network",
        "CNN",
        "Recursive",
        "Neural",
        "Network",
        "RNN",
        "data",
        "visualization",
        "tools",
        "python",
        "Matplotlib",
        "Seaborn",
        "ggplot",
        "pygal",
        "Worked",
        "data",
        "database",
        "sources",
        "Oracle",
        "SQL",
        "Serverand",
        "MongoDB",
        "Hadoop",
        "HBase",
        "Spark",
        "Hive",
        "analysis",
        "extraction",
        "data",
        "infrastructure",
        "data",
        "summarization",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Experience",
        "operating",
        "systems",
        "UNIX",
        "LINUX",
        "Windows",
        "Experience",
        "MS",
        "Word",
        "MS",
        "Excel",
        "MS",
        "PowerPoint",
        "MS",
        "SharePoint",
        "MS",
        "Project",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Zoetis",
        "Inc",
        "September",
        "Present",
        "Zoetis",
        "Inc",
        "worlds",
        "producer",
        "medicine",
        "vaccinations",
        "pets",
        "livestock",
        "Zoetis",
        "quality",
        "medicines",
        "vaccines",
        "products",
        "tests",
        "devices",
        "range",
        "services",
        "project",
        "data",
        "sources",
        "master",
        "data",
        "predictions",
        "sales",
        "Measures",
        "sales",
        "machine",
        "learning",
        "strategies",
        "analyses",
        "animal",
        "health",
        "projects",
        "products",
        "Responsibilities",
        "databases",
        "sources",
        "master",
        "data",
        "data",
        "identification",
        "collection",
        "exploration",
        "cleaning",
        "Performed",
        "Time",
        "Series",
        "Analysis",
        "animal",
        "medicine",
        "vaccine",
        "product",
        "sales",
        "data",
        "order",
        "statistics",
        "characteristics",
        "data",
        "values",
        "values",
        "customer",
        "satisfaction",
        "customer",
        "experience",
        "NLP",
        "Manage",
        "data",
        "sets",
        "variety",
        "sources",
        "analytics",
        "analyses",
        "animal",
        "health",
        "projects",
        "products",
        "Analysis",
        "data",
        "insights",
        "precision",
        "animal",
        "management",
        "precision",
        "medicine",
        "Implementing",
        "analytics",
        "algorithms",
        "Python",
        "Performed",
        "training",
        "Natural",
        "Language",
        "models",
        "reinforcement",
        "learning",
        "engines",
        "agents",
        "task",
        "execution",
        "Performed",
        "kMeans",
        "order",
        "customer",
        "products",
        "customers",
        "customer",
        "products",
        "animal",
        "medicine",
        "vaccines",
        "behavior",
        "information",
        "product",
        "priority",
        "service",
        "relationships",
        "customer",
        "churn",
        "Python",
        "Performed",
        "Text",
        "analytics",
        "email",
        "data",
        "language",
        "processing",
        "tool",
        "kit",
        "NLTK",
        "text",
        "vector",
        "representation",
        "methods",
        "Counter",
        "Vectorizer",
        "Tfidf",
        "topic",
        "algorithms",
        "data",
        "behavior",
        "patterns",
        "data",
        "analytics",
        "team",
        "time",
        "series",
        "optimization",
        "data",
        "PySpark",
        "Machine",
        "library",
        "models",
        "Proof",
        "Concepts",
        "PoC",
        "gap",
        "analysis",
        "data",
        "analysis",
        "sources",
        "data",
        "data",
        "exploration",
        "data",
        "munging",
        "Agile",
        "Methodology",
        "Tableau",
        "reports",
        "records",
        "sources",
        "data",
        "JSON",
        "CSV",
        "support",
        "team",
        "marketing",
        "Environment",
        "Python",
        "PySpark",
        "NLTK",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "SQL",
        "AWS",
        "Tablau",
        "Sqoop",
        "ETL",
        "Data",
        "Scientist",
        "Mars",
        "Solutions",
        "Group",
        "WI",
        "March",
        "August",
        "Client",
        "Healthcare",
        "Provider",
        "health",
        "care",
        "products",
        "insurance",
        "services",
        "Data",
        "Analytics",
        "Payment",
        "Integrity",
        "project",
        "models",
        "customer",
        "value",
        "analysis",
        "machine",
        "learning",
        "methods",
        "component",
        "analysis",
        "regression",
        "data",
        "Responsibilities",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "data",
        "sets",
        "Structured",
        "Unstructured",
        "data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "modeling",
        "Data",
        "Visualization",
        "Performed",
        "Multinomial",
        "Logistic",
        "Regression",
        "Random",
        "forest",
        "machine",
        "algorithms",
        "AWS",
        "data",
        "knowledge",
        "Hadoop",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MapReduce",
        "concepts",
        "Log",
        "files",
        "Python",
        "Alteryx",
        "Data",
        "Preparation",
        "machine",
        "algorithms",
        "Logistic",
        "Regression",
        "Knn",
        "Decision",
        "Trees",
        "Random",
        "Forest",
        "data",
        "output",
        "Interact",
        "brainstorm",
        "teams",
        "opportunities",
        "data",
        "business",
        "healthcare",
        "outcomes",
        "machinelearning",
        "models",
        "ideas",
        "results",
        "management",
        "insights",
        "solutions",
        "businesshealthcare",
        "goals",
        "Design",
        "unit",
        "testing",
        "ETL",
        "package",
        "source",
        "marts",
        "marts",
        "Informatica",
        "ETL",
        "processes",
        "Oracle",
        "database",
        "Agile",
        "Methodology",
        "records",
        "sources",
        "data",
        "JSON",
        "CSV",
        "QlikView",
        "Data",
        "Models",
        "data",
        "sources",
        "DB2",
        "Excel",
        "Flat",
        "Files",
        "data",
        "schedules",
        "status",
        "reports",
        "issue",
        "resolutions",
        "Project",
        "team",
        "Business",
        "Users",
        "Project",
        "Managers",
        "Environment",
        "Python",
        "Linux",
        "Spark",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Spark",
        "SQL",
        "AWS",
        "QlikView",
        "Sqoop",
        "ETL",
        "agile",
        "Data",
        "Analyst",
        "Data",
        "Scientist",
        "CMS",
        "Energy",
        "Jackson",
        "MI",
        "January",
        "February",
        "CMS",
        "Energy",
        "energy",
        "company",
        "utility",
        "operations",
        "data",
        "science",
        "department",
        "help",
        "departments",
        "business",
        "company",
        "competition",
        "machine",
        "learning",
        "algorithm",
        "pricing",
        "strategy",
        "alerts",
        "customers",
        "issues",
        "system",
        "data",
        "Responsibilities",
        "Data",
        "Manipulation",
        "Visualization",
        "Machine",
        "Learning",
        "Python",
        "SQL",
        "Transformed",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "customer",
        "segmentation",
        "learning",
        "technique",
        "Classification",
        "learning",
        "Logistic",
        "Regression",
        "Decision",
        "KNN",
        "Naive",
        "Bayes",
        "models",
        "techniques",
        "Machine",
        "Learning",
        "classification",
        "models",
        "XG",
        "Boost",
        "SVM",
        "Random",
        "Forest",
        "models",
        "accuracy",
        "Gradient",
        "Boosting",
        "technique",
        "Light",
        "GBM",
        "accuracy",
        "Random",
        "Forest",
        "Logistic",
        "Regression",
        "Jupyter",
        "notebook",
        "spark",
        "data",
        "manipulations",
        "ETL",
        "processes",
        "data",
        "conversions",
        "construction",
        "data",
        "warehouse",
        "INFORMATICA",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Interacted",
        "departments",
        "data",
        "needs",
        "requirements",
        "work",
        "members",
        "IT",
        "organization",
        "data",
        "visualization",
        "solutions",
        "needs",
        "Environment",
        "Tableau",
        "AWS",
        "GIT",
        "Python",
        "AnacondaNavigator",
        "Hadoop",
        "NoSQL",
        "forest",
        "MongoDB",
        "HDFS",
        "NLTK",
        "XML",
        "MapReduce",
        "Informatica",
        "Data",
        "Analyst",
        "Karvy",
        "Financial",
        "Services",
        "Limited",
        "November",
        "December",
        "Karvy",
        "Financial",
        "Services",
        "Limited",
        "company",
        "role",
        "growth",
        "India",
        "loans",
        "Micro",
        "Small",
        "Business",
        "segments",
        "individuals",
        "credit",
        "requirements",
        "sectors",
        "economy",
        "Industries",
        "exports",
        "trading",
        "agriculture",
        "infrastructure",
        "segments",
        "projects",
        "customer",
        "Credit",
        "Risk",
        "analysis",
        "risks",
        "loans",
        "loans",
        "fraud",
        "detection",
        "transactions",
        "Responsibilities",
        "data",
        "sources",
        "databases",
        "analysis",
        "data",
        "manipulation",
        "results",
        "concepts",
        "probability",
        "distribution",
        "inference",
        "dataset",
        "findings",
        "comparison",
        "Ttest",
        "Ftest",
        "Rsquared",
        "Pvalue",
        "regression",
        "regression",
        "method",
        "meanvariance",
        "theory",
        "numbers",
        "regression",
        "residuals",
        "Poisson",
        "distribution",
        "Naive",
        "Bayes",
        "function",
        "data",
        "help",
        "Scikit",
        "SciPy",
        "NumPy",
        "Pandas",
        "module",
        "Python",
        "Applied",
        "Principal",
        "Component",
        "Analysis",
        "PCA",
        "technique",
        "VPN",
        "logon",
        "time",
        "Performed",
        "Clustering",
        "data",
        "features",
        "marketing",
        "customers",
        "classification",
        "model",
        "Logistic",
        "Regression",
        "Random",
        "Forests",
        "variable",
        "classes",
        "FScore",
        "Precision",
        "recall",
        "model",
        "performance",
        "user",
        "behavior",
        "models",
        "activity",
        "patterns",
        "risk",
        "scores",
        "transaction",
        "data",
        "learning",
        "models",
        "Decision",
        "trees",
        "Random",
        "Forests",
        "SVM",
        "time",
        "analysis",
        "customer",
        "profile",
        "recommendation",
        "products",
        "Performed",
        "Sentimental",
        "analysis",
        "NLP",
        "email",
        "feedback",
        "customers",
        "tone",
        "series",
        "words",
        "express",
        "attitudes",
        "emotions",
        "LongShort",
        "Term",
        "Memory",
        "LSTM",
        "cells",
        "Recurrent",
        "Neural",
        "NetworksRNN",
        "Forecasted",
        "demand",
        "loans",
        "interest",
        "rates",
        "Time",
        "Series",
        "analysis",
        "ARIMAX",
        "VARMAX",
        "HoltWinters",
        "performance",
        "accuracy",
        "methods",
        "Bootstrap",
        "aggregation",
        "Bagging",
        "Boosting",
        "Light",
        "GBM",
        "Gradient",
        "ETL",
        "mappings",
        "sessions",
        "business",
        "user",
        "requirements",
        "business",
        "rules",
        "data",
        "source",
        "files",
        "RDBMS",
        "tables",
        "tables",
        "data",
        "ingestion",
        "Spark",
        "distribution",
        "environment",
        "Kafka",
        "visualizations",
        "dashboards",
        "ggplot",
        "Tableau",
        "Prepared",
        "data",
        "quality",
        "report",
        "stakeholders",
        "understanding",
        "data",
        "Environment",
        "Tableau",
        "GIT",
        "Python",
        "AnacondaNavigator",
        "Hadoop",
        "Spark",
        "Kafka",
        "NoSQL",
        "Random",
        "forest",
        "MongoDB",
        "HDFS",
        "NLP",
        "Python",
        "Developer",
        "Data",
        "Analyst",
        "Symbiosys",
        "Technologies",
        "Visakhapatnam",
        "Andhra",
        "Pradesh",
        "January",
        "October",
        "Genius",
        "Brands",
        "International",
        "client",
        "data",
        "analysis",
        "purchase",
        "orders",
        "contracts",
        "projects",
        "data",
        "sampling",
        "methods",
        "strata",
        "precision",
        "accuracy",
        "team",
        "members",
        "DBAs",
        "ETL",
        "developers",
        "Technical",
        "Architects",
        "QA",
        "Business",
        "Analysts",
        "Project",
        "Managers",
        "Responsibilities",
        "requirement",
        "gathering",
        "analysis",
        "phase",
        "project",
        "business",
        "requirements",
        "workshopsmeetings",
        "business",
        "users",
        "Python",
        "data",
        "files",
        "Django",
        "Websites",
        "content",
        "files",
        "scripts",
        "Python",
        "stack",
        "APIs",
        "Python",
        "scripts",
        "content",
        "database",
        "manipulate",
        "files",
        "Python",
        "Django",
        "Forms",
        "data",
        "users",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "visualization",
        "tools",
        "Python",
        "R",
        "Hadoop",
        "NumPy",
        "SciPy",
        "matplotlib",
        "Python",
        "machine",
        "learning",
        "algorithms",
        "csv",
        "json",
        "types",
        "files",
        "data",
        "cleaning",
        "data",
        "analysis",
        "R",
        "operations",
        "data",
        "ggplot2",
        "data",
        "Application",
        "ML",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "regression",
        "models",
        "forest",
        "SVM",
        "Volume",
        "package",
        "python",
        "Performed",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "KNN",
        "Naive",
        "Bayes",
        "data",
        "profiling",
        "data",
        "data",
        "sources",
        "data",
        "HDFS",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "data",
        "analysis",
        "data",
        "Performed",
        "time",
        "series",
        "analysis",
        "Tableau",
        "Knowledge",
        "database",
        "platforms",
        "Oracle",
        "DB2",
        "NoSQL",
        "projects",
        "work",
        "hour",
        "productivity",
        "cycle",
        "Environment",
        "Python",
        "Django",
        "R",
        "Oracle",
        "GitHub",
        "SQL",
        "Server",
        "HDFS",
        "Hive",
        "ETL",
        "Developer",
        "Sutherland",
        "Global",
        "Services",
        "Hyderabad",
        "Telangana",
        "February",
        "December",
        "Sutherland",
        "processes",
        "age",
        "speed",
        "insight",
        "design",
        "thinking",
        "scale",
        "accuracy",
        "data",
        "analytics",
        "Sutherland",
        "customers",
        "industries",
        "services",
        "Healthcare",
        "role",
        "Analytics",
        "department",
        "data",
        "extraction",
        "cleaning",
        "data",
        "steps",
        "models",
        "Responsibilities",
        "Business",
        "Analysts",
        "team",
        "requirements",
        "specifications",
        "specifications",
        "Data",
        "mapping",
        "specifications",
        "system",
        "test",
        "data",
        "mapping",
        "specifies",
        "data",
        "data",
        "warehouse",
        "entity",
        "SDLC",
        "processes",
        "requirements",
        "management",
        "workflow",
        "analysis",
        "source",
        "data",
        "analysis",
        "data",
        "mapping",
        "metadata",
        "management",
        "data",
        "quality",
        "testing",
        "strategy",
        "maintenance",
        "model",
        "ETLs",
        "data",
        "sources",
        "applications",
        "PLSQL",
        "SSIS",
        "packages",
        "transform",
        "data",
        "SQL",
        "Server",
        "lots",
        "components",
        "SSIS",
        "Pivot",
        "Transformation",
        "Fuzzy",
        "Lookup",
        "Merge",
        "Merge",
        "Join",
        "Data",
        "Conversion",
        "Row",
        "Count",
        "Columns",
        "Conditional",
        "Split",
        "Execute",
        "SQL",
        "Task",
        "Data",
        "Flow",
        "Task",
        "Execute",
        "Package",
        "Task",
        "SSIS",
        "Packages",
        "source",
        "files",
        "Excel",
        "XML",
        "destination",
        "formats",
        "ETL",
        "packages",
        "breakpoint",
        "process",
        "error",
        "information",
        "SQL",
        "command",
        "SSIS",
        "Developed",
        "SQL",
        "queries",
        "SQL",
        "Server",
        "management",
        "studio",
        "Toad",
        "reports",
        "users",
        "reporting",
        "processes",
        "UNIX",
        "shell",
        "scripting",
        "utilities",
        "MLOAD",
        "BTEQ",
        "Fast",
        "Load",
        "Experience",
        "Perl",
        "Performed",
        "data",
        "analysis",
        "data",
        "profiling",
        "SQL",
        "sources",
        "systems",
        "Oracle",
        "Teradata",
        "Environment",
        "ETL",
        "Tools",
        "SDLC",
        "GitHub",
        "SQL",
        "Server",
        "PLSQL",
        "Excel",
        "XML",
        "SQL",
        "Education",
        "Master",
        "Science",
        "Information",
        "Technology",
        "Management",
        "Information",
        "Technology",
        "Management",
        "Campbellsville",
        "University",
        "Bachelors",
        "Electronics",
        "Communication",
        "Engineering",
        "Electronics",
        "Communication",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:30:09.440256",
    "resume_data": "Data Scientist Data Scientist Data Scientist Zoetis Inc Data Scientist with 6 Plus years of experience in Statistical Modeling Machine Learning Data Mining with Structured and Unstructured Data Performed Data Acquisition Data Validation Predictive Modeling and Data Visualization Expertise in Python 2x3x programming with multiple packages including NumPy Pandas Matplotlib SciPy Seaborn and Scikitlearn Hands on experience in implementing LDA Nave Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering Neural Networks Principle Component Analysis and good knowledge on Recommender Systems Implementation experiences in Machine Learning and deep learning including Regression Classification Natural Language Processing NLP using packages like NLTK Spacy Experience in tuning algorithms using methods such as Grid Search Randomized Search KFold Cross Validation and Error Analysis Also worked with several boosting methodologies like ADA Boost Gradient Boosting and XGBoost Validated the machine learning classifiers using Accuracy AUC ROC Curves and Lift Charts Worked with various text analytics or Word Embedding libraries like Word2Vec Count Vectorizer GloVe LDA etc Solid knowledge and experience in Deep Learning techniques including Feedforward Neural Network Convolutional Neural Network CNN Recursive Neural Network RNN Worked with numerous data visualization tools in python like Matplotlib Seaborn ggplot pygal Worked and extracted data from various database sources like Oracle SQL Serverand MongoDB Highly skilled in using Hadoop HBase Spark and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Experience on working with different operating systems like UNIX LINUX and Windows Experience working with MS Word MS Excel MS PowerPoint MS SharePoint and MS Project Work Experience Data Scientist Zoetis Inc September 2018 to Present Zoetis Inc is the worlds largest producer of medicine and vaccinations for pets and livestock Zoetis delivers quality medicines vaccines and diagnostic products which are complemented by genetic tests bio devices and a range of services The project is to collect data from different sources and create a master data set and doing predictions on sales and profits Measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products Responsibilities Developing analytical databases from different sources and create a master data set Responsible for data identification collection exploration cleaning for modeling Performed Time Series Analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values Determined customer satisfaction and helped enhance customer experience using NLP Manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products Analysis of biological and spatial data to develop insights into precision animal management and precision medicine Implementing analytics algorithms in Python Performed training Natural Language models and reinforcement learning engines to optimize intelligent agents that automate task execution Performed kMeans clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering customized and priority service to improve existing profitable relationships and to avoid customer churn etc using Python Performed Text analytics on unstructured email data using Natural language processing tool kit NLTK Worked with text to vector representation methods including Counter Vectorizer Tfidf for topic modelling Applying Clustering algorithms to group the data on their similar behavior patterns Work with data analytics team to develop time series and optimization Experienced in data scraping Used PySpark Machine learning library to build and evaluate different models Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Experienced in Agile Methodology Used Tableau to generate reports with internal records secondary sources of data JSON CSV and more Which helped the support team for better marketing Environment Python 370 PySpark NLTK SQL Server Microsoft Excel SQL AWS Tablau Sqoop ETL agile Data Scientist Mars Solutions Group WI March 2017 to August 2018 The Client is the largest Healthcare Provider and offers health care products insurance services Data Analytics Payment Integrity and The project was to build predictive models for customer value analysis by applying machine learning methods principal component analysis and regression on large data set Responsibilities Worked on Machine Learning Data mining with large data sets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Performed Multinomial Logistic Regression Random forest machine learning algorithms Used AWS to manage the data in cloud Good knowledge on Hadoop components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Maintained updated Log files using Python Alteryx was used for Data Preparation Used machine learning algorithms like Logistic Regression Knn Decision Trees Random Forest to make the data to fit for the desired output Interact and brainstorm with multifunctional teams to explore the opportunities of using data to improve business and healthcare outcomes Developed machinelearning models and translate complex ideas and results into actionable management insights and solutions to achieve the expected businesshealthcare goals Design coding unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database Worked in Agile Methodology Generated reports with internal records secondary sources of data JSON CSV and more Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Provided schedules status reports and issue resolutions to the Project team Business Users and Project Managers Environment Python 3x Linux Spark SQL Server 2012 Microsoft Excel Spark SQL AWS QlikView Sqoop ETL agile Data Analyst Data Scientist CMS Energy Jackson MI January 2016 to February 2017 CMS Energy is an energy company that is focused principally on utility operations I was responsible for building a new data science department with the help of other departments and I was able to learn how the business is operated and helped the company to grow and stay ahead of the competition By using machine learning we improvised the predictive algorithm for pricing strategy And we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me Responsibilities Worked on Data Manipulation Visualization Machine Learning Python and SQL Transformed the business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Worked on customer segmentation using an unsupervised learning technique clustering Implemented Classification using Supervised learning like Logistic Regression Decision trees KNN Naive Bayes Built models using Statistical techniques and Machine Learning classification models like XG Boost SVM and Random Forest Improved models accuracy by using Gradient Boosting technique like Light GBM and gained around 82 accuracy with Random Forest and 77 with Logistic Regression Used Jupyter notebook for spark to make data manipulations Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs Environment Tableau 1005 AWS GIT Python 352 AnacondaNavigator Hadoop NoSQL random forest MongoDB HDFS NLTK XML MapReduce Informatica Data Analyst Karvy Financial Services Limited November 2014 to December 2015 Karvy Financial Services Limited is a company which has been playing a very proactive role in the economic growth of India by providing loans to Micro Small Business segments and individuals like credit for the requirements of different sectors of economy Industries exports trading agriculture infrastructure and the individual segments We worked on various projects which handles customer analytics Credit Risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans identify and prevent fraud detection for transactions Responsibilities Compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results Applied concepts of probability distribution and statistical inference on the given dataset to unearth interesting findings using comparison Ttest Ftest Rsquared Pvalue etc Applied linear regression multiple regression ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied Principal Component Analysis PCA based unsupervised technique to determine unusual VPN logon time Performed Clustering with historical demographic and behavioral data as features to implement the personalized marketing to the customers Also created classification model using Logistic Regression Random Forests to classify dependent variable into two classes which are risky and okay Used FScore Precision recall evaluating model performance Built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as Decision trees Random Forests and SVM Real time analysis of customer financial profile and providing recommendation for financial products best suited Performed Sentimental analysis NLP on the email feedback of the customers to determine the emotional tone behind the series of words and gain the express of the attitudes and emotions by LongShort Term Memory LSTM cells in Recurrent Neural NetworksRNN Forecasted demand for loans and interest rates using Time Series analysis like ARIMAX VARMAX and HoltWinters Obtained better predictive performance of 81 accuracy using ensemble methods like Bootstrap aggregation Bagging and Boosting Light GBM Gradient Tested complex ETL mappings and sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Performed financial data ingestion to the Spark distribution environment using Kafka Developed visualizations and dashboards using ggplot Tableau Prepared and presented data quality report to stakeholders to give understanding of data Environment Tableau 1005 GIT Python 352 AnacondaNavigator Hadoop Spark Kafka NoSQL Random forest MongoDB HDFS NLP Python Developer Data Analyst Symbiosys Technologies Visakhapatnam Andhra Pradesh January 2014 to October 2014 Genius Brands International is our client and we performed exploratory data analysis on corporate purchase orders contracts and projects data using sampling and statistical methods Identified strata improved precision and accuracy Works with other team members including DBAs Other ETL developers Technical Architects QA and Business Analysts Project Managers Responsibilities Participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Used Python to place data into JSON files for testing Django Websites Updated and manipulated content and files by using python scripts Worked on Python Open stack APIs Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Implemented endtoend systems for Data Analytics Data Automation and customized visualization tools using Python R Hadoop and MongoDB Used pandas NumPy seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Worked on csv json excel different types of files for the data cleaning and data analysis Used R for statistical operations on the data and ggplot2 for the visualizing the data Application of various ML algorithms and statistical modeling like decision trees regression models random forest SVM clustering to identify Volume using scikitlearn package in python Performed Classification using Supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Performed data profiling to merge the data from multiple data sources Extracted data from HDFS Hadoop Distributed File System and prepared data for exploratory analysis using data munging Performed time series analysis using Tableau Knowledge of other relational database platforms such as Oracle DB2 NoSQL Managed offshore projects and coordinated work for 24 hour productivity cycle Environment Python 27 Django 14 R Oracle GitHub SQL Server HDFS Hive ETL Developer Sutherland Global Services Hyderabad Telangana February 2013 to December 2013 Sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics Sutherland has customers across industries like financial services to Healthcare My role is to assist Analytics department for the data extraction and cleaning as a data preprocessing steps to build models Responsibilities Involved with Business Analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Managed full SDLC processes involving requirements management workflow analysis source data analysis data mapping metadata management data quality testing strategy and maintenance of the model Developed ETLs to pull data from various sources and transform it for reporting applications using PLSQL Designed SSIS packages to extract transform and load existing data into SQL Server used lots of components of SSIS such as Pivot Transformation Fuzzy Lookup Merge Merge Join Data Conversion Row Count Sort Derived Columns Conditional Split Execute SQL Task Data Flow Task and Execute Package Task Created SSIS Packages that involved dealing with different source formats flat files Excel XML and different destination formats Debugged and troubleshot the ETL packages by using a breakpoint analyzing the process catching error information by SQL command in SSIS Developed SQL queries in SQL Server management studio Toad and generated complex reports forth end users Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD BTEQ and Fast Load Experience with Perl Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Environment ETL Tools SDLC GitHub SQL Server PLSQL Excel XML SQL Education Master of Science in Information Technology Management in Information Technology Management Campbellsville University Bachelors in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University",
    "unique_id": "5c7f2f61-c028-457c-82b0-2b517c397f17"
}